{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71310acb",
   "metadata": {},
   "source": [
    "## Processing Parallel Corpora\n",
    "\n",
    "The entire data processing in this project is built on the **datatrove** library from **huggingface**.\n",
    "The main storage for one data sample in datatrove is the Document class:\n",
    "\n",
    "```python\n",
    "class Document:\n",
    "    text: str\n",
    "    id: str\n",
    "    media: list[Media] = field(default_factory=list)\n",
    "    metadata: dict[str, Any] = field(default_factory=dict)\n",
    "```\n",
    "\n",
    "When you start working, you specify which column of your dataset you want to work with, after which this column is renamed to \"text\", and all other data goes into Document.metadata.\n",
    "\n",
    "**Data Transformation Example:**\n",
    "\n",
    "Original Dataset:\n",
    "```python\n",
    "{\n",
    "    \"ru\": \"ĞŸÑ€Ğ¸Ğ²ĞµÑ‚, Ğ¼Ğ¸Ñ€!\",\n",
    "    \"chv\": \"Ğ¡Ñ‹Ğ²Ğ»ÄƒÑ…, Ñ‚Ä•Ğ½Ñ‡Ğµ!\",\n",
    "    \"random_shit\": 42\n",
    "}\n",
    "```\n",
    "\n",
    "When selecting \"ru\" as the key column:\n",
    "\n",
    "```python\n",
    "doc.text = \"ĞŸÑ€Ğ¸Ğ²ĞµÑ‚, Ğ¼Ğ¸Ñ€!\"\n",
    "doc.metadata = {\n",
    "    'chv': \"Ğ¡Ñ‹Ğ²Ğ»ÄƒÑ…, Ñ‚Ä•Ğ½Ñ‡Ğµ!\",\n",
    "    'random_shit': 42\n",
    "}\n",
    "```\n",
    "\n",
    "Visual representation:\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      Original Data    â”‚                 â”‚          Document             â”‚\n",
    "â”‚                       â”‚                 â”‚                               â”‚\n",
    "â”‚  \"ru\": \"ĞŸÑ€Ğ¸Ğ²ĞµÑ‚, Ğ¼Ğ¸Ñ€!\" â”‚â”€â”€â”€â”€â”€â”           â”‚  text: \"ĞŸÑ€Ğ¸Ğ²ĞµÑ‚, Ğ¼Ğ¸Ñ€!\"         â”‚\n",
    "â”‚                       â”‚     â”‚           â”‚                               â”‚\n",
    "â”‚  \"chv\": \"Ğ¡Ñ‹Ğ²Ğ»ÄƒÑ…, ...\" â”‚     â”‚           â”‚  metadata: {                  â”‚\n",
    "â”‚                       â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚    \"chv\": \"Ğ¡Ñ‹Ğ²Ğ»ÄƒÑ…, Ñ‚Ä•Ğ½Ñ‡Ğµ!\",   â”‚\n",
    "â”‚  \"random_shit\": 42    â”‚                 â”‚    \"random_shit\": 42          â”‚\n",
    "â”‚                       â”‚                 â”‚  }                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "```\n",
    "\n",
    "If you want to work with multiple columns simultaneously, datatrove in most cases cannot provide you with such an opportunity - in some basic classes, for example, it is hardcoded that work is done exclusively on Document.text.\n",
    "\n",
    "Eeve implements, based on some blocks of datatrove, such blocks that allow you to directly specify which column from your dataset you would like to transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e96982e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from datatrove.pipeline.readers.csv import CsvReader\n",
    "from datatrove.pipeline.writers.jsonl import JsonlWriter\n",
    "from datatrove.executor.local import LocalPipelineExecutor\n",
    "\n",
    "from eeve.utils.datatrove import (\n",
    "    _reader_adapter_with_column_info,\n",
    "    _writer_adapter_with_column_restore\n",
    ")\n",
    "\n",
    "from eeve.data.formatters.callable import CallableFormatter\n",
    "from eeve.data.formatters.quote import QuoteReplacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "807433d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_random_rows(df_list, k):\n",
    "    m = min(len(df) for df in df_list)\n",
    "    idx = random.sample(range(m), k)\n",
    "    n = min(df.shape[1] for df in df_list)\n",
    "    a = df_list[0]\n",
    "    b = df_list[1] if len(df_list) == 2 else None\n",
    "    for c in range(n):\n",
    "        print(list(a.iloc[idx, c]))\n",
    "        if b is not None:\n",
    "            print(list(b.iloc[idx, c]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d941174e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That professor built \"the historical\" document.</td>\n",
       "      <td>The cat mysteriously \"built the\" ancient artif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The   government   explained   the   natural  ...</td>\n",
       "      <td>Our team discovered    the   interesting    th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The  beautiful    the company walked    the a...</td>\n",
       "      <td>This computer powerfully designed the \"natural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A    student    powerfully  investigated the  ...</td>\n",
       "      <td>The  interesting  this   computer  walked   t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A    student   quickly  discovered  an innova...</td>\n",
       "      <td>Our team investigated a    challenging  puzzle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Several birds interestingly \"created the natur...</td>\n",
       "      <td>The powerful that  professor  created    a  ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The cat studied the \"ancient artifact quickly.\"</td>\n",
       "      <td>My   neighbor   created  the historical   docu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_1  \\\n",
       "0    That professor built \"the historical\" document.   \n",
       "1  The   government   explained   the   natural  ...   \n",
       "2   The  beautiful    the company walked    the a...   \n",
       "3  A    student    powerfully  investigated the  ...   \n",
       "4   A    student   quickly  discovered  an innova...   \n",
       "5  Several birds interestingly \"created the natur...   \n",
       "6    The cat studied the \"ancient artifact quickly.\"   \n",
       "\n",
       "                                              text_2  \n",
       "0  The cat mysteriously \"built the\" ancient artif...  \n",
       "1  Our team discovered    the   interesting    th...  \n",
       "2  This computer powerfully designed the \"natural...  \n",
       "3   The  interesting  this   computer  walked   t...  \n",
       "4  Our team investigated a    challenging  puzzle...  \n",
       "5  The powerful that  professor  created    a  ch...  \n",
       "6  My   neighbor   created  the historical   docu...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = pd.read_csv(\"./files/parallel_corpus_process.csv\")\n",
    "inputs.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92205a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That professor built \"the historical\" document.', '\"The government\" developed the mathematical equation.']\n",
      "\n",
      "['The cat mysteriously \"built the\" ancient artifact.', 'A student    walked    the  amazing    the   complex    problem.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_random_rows([inputs], k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dde32d",
   "metadata": {},
   "source": [
    "To get a dataset after transformations not in the format that datatrove provides, but in the original format (preserving all columns and their names), you can use adapters. The _reader_adapter_with_column_info adapter is used when reading data and saves information about the original columns and their names in metadata, while the _writer_adapter_with_column_restore adapter takes this information into account when writing data.\n",
    "\n",
    "NB: to get data in the same format at the output as you submitted at the input, you need to use **both reader and writer adapters** in the pipeline. An example will be shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21455392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-19 22:34:46.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file , 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(text='That professor built \"the historical\" document.', id='/0', media=[], metadata={'text_2': 'The cat mysteriously \"built the\" ancient artifact.', 'file_path': 'd:/vs_projects/eeve/examples/files/parallel_corpus_process.csv'})\n"
     ]
    }
   ],
   "source": [
    "example_reader = CsvReader(\"./files/parallel_corpus_process.csv\", text_key=\"text_1\")\n",
    "gen = example_reader.run()\n",
    "\n",
    "doc = next(iter(gen))\n",
    "print(doc) # as we can see, all values from the dataset, except text_key='text_1', have been transferred to doc.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb406dd",
   "metadata": {},
   "source": [
    "In the demonstration dataset above, we can see several text formatting issues:\n",
    "- chevron quotes need to be properly placed;\n",
    "- multiple spaces need to be replaced;\n",
    "- there are indentations before the first or after the last character in a sentence.\n",
    "\n",
    "Let's implement functions that remove these deficiencies, and use them as examples to transform the columns in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67f4e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(s: str) -> str:\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aee03825",
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = LocalPipelineExecutor(\n",
    "    pipeline=[\n",
    "        CsvReader(\n",
    "            \"./files/parallel_corpus_process.csv\",\n",
    "            text_key=\"text_1\",\n",
    "            adapter=_reader_adapter_with_column_info\n",
    "        ),\n",
    "        CallableFormatter(func=process_string, list_path=[\"text\", \"metadata['text_2']\"]),\n",
    "        QuoteReplacer(list_path=[\"text\", \"metadata['text_2']\"]),\n",
    "        JsonlWriter(\n",
    "            output_folder=\"./files\",\n",
    "            output_filename='result_${rank}',\n",
    "            adapter=_writer_adapter_with_column_restore\n",
    "        )\n",
    "    ],\n",
    "    tasks=1,\n",
    "    workers=1,\n",
    "    skip_completed=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03154ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-19 22:34:51.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.utils.logging\u001b[0m:\u001b[36madd_task_logger\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mLaunching pipeline for rank=0\u001b[0m\n",
      "\u001b[32m2025-08-19 22:34:51.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.utils.logging\u001b[0m:\u001b[36mlog_pipeline\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1m\n",
      "--- ğŸ› ï¸ PIPELINE ğŸ› \n",
      "ğŸ“– - READER: ğŸ”¢ Csv\n",
      "âœ‚ï¸ - FORMAT: ğŸ”§ Custom\n",
      "âœ‚ï¸ - FORMAT: ğŸ’¬ Quotes\n",
      "ğŸ’½ - WRITER: ğŸ¿ Jsonl\u001b[0m\n",
      "--- Logging error in Loguru Handler #3 ---\n",
      "Record was: {'elapsed': datetime.timedelta(seconds=7, microseconds=938448), 'exception': None, 'extra': {}, 'file': (name='logging.py', path='c:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\datatrove\\\\utils\\\\logging.py'), 'function': 'log_pipeline', 'level': (name='INFO', no=20, icon='â„¹ï¸'), 'line': 90, 'message': '\\n--- ğŸ› ï¸ PIPELINE ğŸ› \\nğŸ“– - READER: ğŸ”¢ Csv\\nâœ‚ï¸ - FORMAT: ğŸ”§ Custom\\nâœ‚ï¸ - FORMAT: ğŸ’¬ Quotes\\nğŸ’½ - WRITER: ğŸ¿ Jsonl', 'module': 'logging', 'name': 'datatrove.utils.logging', 'process': (id=1416, name='MainProcess'), 'thread': (id=21032, name='MainThread'), 'time': datetime(2025, 8, 19, 22, 34, 51, 37312, tzinfo=datetime.timezone(datetime.timedelta(seconds=10800), 'RTZ 2 (Ğ·Ğ¸Ğ¼Ğ°)'))}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\loguru\\_handler.py\", line 206, in emit\n",
      "    self._sink.write(str_record)\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\loguru\\_simple_sinks.py\", line 16, in write\n",
      "    self._stream.write(message)\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\encodings\\cp1251.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode characters in position 85-86: character maps to <undefined>\n",
      "--- End of logging error ---\n",
      "\u001b[32m2025-08-19 22:34:51.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file , 1/1\u001b[0m\n",
      "\u001b[32m2025-08-19 22:34:51.059\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdatatrove.executor.base\u001b[0m:\u001b[36m_run_for_rank\u001b[0m:\u001b[36m98\u001b[0m - \u001b[32m\u001b[1mProcessing done for rank=0\u001b[0m\n",
      "\u001b[32m2025-08-19 22:34:51.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.executor.base\u001b[0m:\u001b[36m_run_for_rank\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1m\n",
      "\n",
      "ğŸ“‰ğŸ“‰ğŸ“‰ Stats: Task 0 ğŸ“‰ğŸ“‰ğŸ“‰\n",
      "\n",
      "Total Runtime: 0 seconds\n",
      "\n",
      "ğŸ“– - READER: ğŸ”¢ Csv\n",
      "    Runtime: (6.50%) 0 seconds [0.02 millisecondsÂ±0.01 milliseconds/doc]\n",
      "    Stats: {input_files: 1, doc_len: 2730 [min=31, max=76, 54.60Â±9/doc], documents: 50 [50.00/input_file]}\n",
      "âœ‚ï¸ - FORMAT: ğŸ”§ Custom\n",
      "    Runtime: (24.34%) 0 seconds [0.06 millisecondsÂ±0.03 milliseconds/doc]\n",
      "    Stats: {total: 50}\n",
      "âœ‚ï¸ - FORMAT: ğŸ’¬ Quotes\n",
      "    Runtime: (21.65%) 0 seconds [0.06 millisecondsÂ±0.02 milliseconds/doc]\n",
      "    Stats: {total: 50}\n",
      "ğŸ’½ - WRITER: ğŸ¿ Jsonl\n",
      "    Runtime: (47.51%) 0 seconds [0.12 millisecondsÂ±0.49 milliseconds/doc]\n",
      "    Stats: {result_XXXXX.gz: 50, total: 50, doc_len: 2537 [min=30, max=62, 50.74Â±7/doc]}\u001b[0m\n",
      "--- Logging error in Loguru Handler #3 ---\n",
      "Record was: {'elapsed': datetime.timedelta(seconds=7, microseconds=963440), 'exception': None, 'extra': {}, 'file': (name='base.py', path='c:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\datatrove\\\\executor\\\\base.py'), 'function': '_run_for_rank', 'level': (name='INFO', no=20, icon='â„¹ï¸'), 'line': 104, 'message': '\\n\\nğŸ“‰ğŸ“‰ğŸ“‰ Stats: Task 0 ğŸ“‰ğŸ“‰ğŸ“‰\\n\\nTotal Runtime: 0 seconds\\n\\nğŸ“– - READER: ğŸ”¢ Csv\\n    Runtime: (6.50%) 0 seconds [0.02 millisecondsÂ±0.01 milliseconds/doc]\\n    Stats: {input_files: 1, doc_len: 2730 [min=31, max=76, 54.60Â±9/doc], documents: 50 [50.00/input_file]}\\nâœ‚ï¸ - FORMAT: ğŸ”§ Custom\\n    Runtime: (24.34%) 0 seconds [0.06 millisecondsÂ±0.03 milliseconds/doc]\\n    Stats: {total: 50}\\nâœ‚ï¸ - FORMAT: ğŸ’¬ Quotes\\n    Runtime: (21.65%) 0 seconds [0.06 millisecondsÂ±0.02 milliseconds/doc]\\n    Stats: {total: 50}\\nğŸ’½ - WRITER: ğŸ¿ Jsonl\\n    Runtime: (47.51%) 0 seconds [0.12 millisecondsÂ±0.49 milliseconds/doc]\\n    Stats: {result_XXXXX.gz: 50, total: 50, doc_len: 2537 [min=30, max=62, 50.74Â±7/doc]}', 'module': 'base', 'name': 'datatrove.executor.base', 'process': (id=1416, name='MainProcess'), 'thread': (id=21032, name='MainThread'), 'time': datetime(2025, 8, 19, 22, 34, 51, 62304, tzinfo=datetime.timezone(datetime.timedelta(seconds=10800), 'RTZ 2 (Ğ·Ğ¸Ğ¼Ğ°)'))}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\loguru\\_handler.py\", line 206, in emit\n",
      "    self._sink.write(str_record)\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\loguru\\_simple_sinks.py\", line 16, in write\n",
      "    self._stream.write(message)\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\encodings\\cp1251.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode characters in position 85-87: character maps to <undefined>\n",
      "--- End of logging error ---\n",
      "\u001b[32m2025-08-19 22:34:51.069\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdatatrove.executor.local\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m148\u001b[0m - \u001b[32m\u001b[1m\n",
      "\n",
      "ğŸ“‰ğŸ“‰ğŸ“‰ Stats: All 1 tasks ğŸ“‰ğŸ“‰ğŸ“‰\n",
      "\n",
      "Total Runtime: 0 seconds\n",
      "\n",
      "ğŸ“– - READER: ğŸ”¢ Csv\n",
      "    Runtime: (6.50%) 0 seconds [0.02 millisecondsÂ±0.01 milliseconds/doc]\n",
      "    Stats: {input_files: 1, doc_len: 2730 [min=31, max=76, 54.60Â±9/doc], documents: 50 [50.00/input_file]}\n",
      "âœ‚ï¸ - FORMAT: ğŸ”§ Custom\n",
      "    Runtime: (24.34%) 0 seconds [0.06 millisecondsÂ±0.03 milliseconds/doc]\n",
      "    Stats: {total: 50}\n",
      "âœ‚ï¸ - FORMAT: ğŸ’¬ Quotes\n",
      "    Runtime: (21.65%) 0 seconds [0.06 millisecondsÂ±0.02 milliseconds/doc]\n",
      "    Stats: {total: 50}\n",
      "ğŸ’½ - WRITER: ğŸ¿ Jsonl\n",
      "    Runtime: (47.51%) 0 seconds [0.12 millisecondsÂ±0.49 milliseconds/doc]\n",
      "    Stats: {result_XXXXX.gz: 50, total: 50, doc_len: 2537 [min=30, max=62, 50.74Â±7/doc]}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "ğŸ“‰ğŸ“‰ğŸ“‰ Stats ğŸ“‰ğŸ“‰ğŸ“‰\n",
       "\n",
       "Total Runtime: 0 seconds\n",
       "\n",
       "ğŸ“– - READER: ğŸ”¢ Csv\n",
       "    Runtime: (6.50%) 0 seconds [0.02 millisecondsÂ±0.01 milliseconds/doc]\n",
       "    Stats: {input_files: 1, doc_len: 2730 [min=31, max=76, 54.60Â±9/doc], documents: 50 [50.00/input_file]}\n",
       "âœ‚ï¸ - FORMAT: ğŸ”§ Custom\n",
       "    Runtime: (24.34%) 0 seconds [0.06 millisecondsÂ±0.03 milliseconds/doc]\n",
       "    Stats: {total: 50}\n",
       "âœ‚ï¸ - FORMAT: ğŸ’¬ Quotes\n",
       "    Runtime: (21.65%) 0 seconds [0.06 millisecondsÂ±0.02 milliseconds/doc]\n",
       "    Stats: {total: 50}\n",
       "ğŸ’½ - WRITER: ğŸ¿ Jsonl\n",
       "    Runtime: (47.51%) 0 seconds [0.12 millisecondsÂ±0.49 milliseconds/doc]\n",
       "    Stats: {result_XXXXX.gz: 50, total: 50, doc_len: 2537 [min=30, max=62, 50.74Â±7/doc]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "494f5565",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_json(path_or_buf='./files/result_00000.gz', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f31cf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Several birds interestingly \"created the natural\" phenomenon.', 'Several   birds    analyzed an   innovative solution.']\n",
      "['Several birds interestingly Â«created the naturalÂ» phenomenon.', 'Several birds analyzed an innovative solution.']\n",
      "\n",
      "['The powerful that  professor  created    a  challenging puzzle. ', ' The colorful the company analyzed the complex problem.']\n",
      "['The powerful that professor created a challenging puzzle.', 'The colorful the company analyzed the complex problem.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_random_rows([inputs, result], k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e40de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
